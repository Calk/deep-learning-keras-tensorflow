{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural networks\n",
    "====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<img src =\"imgs/rnn.png\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "output_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.recurrent.SimpleRNN at 0x7f52d88b7d30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.recurrent.SimpleRNN(output_dim, \n",
    "                                 init='glorot_uniform', inner_init='orthogonal', activation='tanh', \n",
    "                                 W_regularizer=None, U_regularizer=None, b_regularizer=None, \n",
    "                                 dropout_W=0.0, dropout_U=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backprop Through time  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrary to feed-forward neural networks, the RNN is characterized by the ability of encoding longer past information, thus very suitable for sequential models. The BPTT extends the ordinary BP algorithm to suit the recurrent neural\n",
    "architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "<img src =\"imgs/rnn2.png\" width=\"45%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.preprocessing import image\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.layers.core import Activation, TimeDistributedDense, RepeatVector\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.python.control_flow_ops = tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB sentiment classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. \n",
    "\n",
    "IMDB provided a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. \n",
    "\n",
    "There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. \n",
    "\n",
    "http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation - IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Example:\n",
      "[ [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]]\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (25000, 100)\n",
      "X_test shape: (25000, 100)\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "maxlen = 100  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print(\"Loading data...\")\n",
    "# (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features, test_split=0.2)\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print('Example:')\n",
    "print(X_train[:1])\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "max_epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 61s - loss: 0.7009 - acc: 0.5455 - val_loss: 0.5840 - val_acc: 0.6860\n",
      "24992/25000 [============================>.] - ETA: 0s\n",
      "Test accuracy: 0.583960152206\n",
      "Test loss: 0.68604\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(SimpleRNN(128))  \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"Train...\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=1, validation_data=(X_test, y_test))\n",
    "acc , loss = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print()\n",
    "print('Test accuracy:', acc)\n",
    "print('Test loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A LSTM network is an artificial neural network that contains LSTM blocks instead of, or in addition to, regular network units. A LSTM block may be described as a \"smart\" network unit that can remember a value for an arbitrary length of time. \n",
    "\n",
    "Unlike traditional RNNs, an Long short-term memory network is well-suited to learn from experience to classify, process and predict time series when there are very long time lags of unknown size between important events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "<img src =\"imgs/gru.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.recurrent.LSTM at 0x7fafce6adf60>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.recurrent.LSTM(output_dim, init='glorot_uniform', inner_init='orthogonal', \n",
    "                            forget_bias_init='one', activation='tanh', \n",
    "                            inner_activation='hard_sigmoid', \n",
    "                            W_regularizer=None, U_regularizer=None, b_regularizer=None, \n",
    "                            dropout_W=0.0, dropout_U=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gated recurrent units are a gating mechanism in recurrent neural networks. \n",
    "\n",
    "Much similar to the LSTMs, they have fewer parameters than LSTM, as they lack an output gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.recurrent.GRU at 0x7fafce6adf28>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.recurrent.GRU(output_dim, init='glorot_uniform', inner_init='orthogonal', \n",
    "                           activation='tanh', inner_activation='hard_sigmoid', \n",
    "                           W_regularizer=None, U_regularizer=None, b_regularizer=None, \n",
    "                           dropout_W=0.0, dropout_U=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn! - Hands on Rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/50\n",
      "25000/25000 [==============================] - 60s - loss: 0.7131 - acc: 0.5300 - val_loss: 0.6646 - val_acc: 0.5749\n",
      "Epoch 2/50\n",
      "25000/25000 [==============================] - 61s - loss: 0.5396 - acc: 0.7244 - val_loss: 1.0878 - val_acc: 0.6199\n",
      "Epoch 3/50\n",
      "25000/25000 [==============================] - 60s - loss: 0.4727 - acc: 0.7913 - val_loss: 0.4992 - val_acc: 0.7603\n",
      "Epoch 4/50\n",
      "25000/25000 [==============================] - 60s - loss: 0.4305 - acc: 0.8233 - val_loss: 0.4692 - val_acc: 0.7978\n",
      "Epoch 5/50\n",
      "25000/25000 [==============================] - 60s - loss: 0.3811 - acc: 0.8468 - val_loss: 0.4556 - val_acc: 0.7897\n",
      "Epoch 6/50\n",
      "25000/25000 [==============================] - 60s - loss: 0.3603 - acc: 0.8505 - val_loss: 0.4583 - val_acc: 0.8065\n",
      "Epoch 7/50\n",
      "25000/25000 [==============================] - 101s - loss: 0.4698 - acc: 0.7655 - val_loss: 0.6272 - val_acc: 0.6393\n",
      "Epoch 8/50\n",
      "25000/25000 [==============================] - 90s - loss: 0.4426 - acc: 0.8039 - val_loss: 0.5575 - val_acc: 0.7612\n",
      "Epoch 9/50\n",
      "25000/25000 [==============================] - 66s - loss: 0.3724 - acc: 0.8520 - val_loss: 0.5172 - val_acc: 0.7542\n",
      "Epoch 10/50\n",
      "25000/25000 [==============================] - 62s - loss: 0.3318 - acc: 0.8692 - val_loss: 0.5113 - val_acc: 0.7936\n",
      "Epoch 11/50\n",
      "25000/25000 [==============================] - 62s - loss: 0.2648 - acc: 0.9014 - val_loss: 0.5171 - val_acc: 0.7616\n",
      "Epoch 12/50\n",
      "25000/25000 [==============================] - 76s - loss: 0.3281 - acc: 0.8640 - val_loss: 0.6576 - val_acc: 0.6696\n",
      "Epoch 13/50\n",
      "25000/25000 [==============================] - 126s - loss: 0.5103 - acc: 0.7619 - val_loss: 0.6246 - val_acc: 0.6517\n",
      "Epoch 14/50\n",
      "25000/25000 [==============================] - 136s - loss: 0.4711 - acc: 0.7816 - val_loss: 0.6375 - val_acc: 0.6316\n",
      "Epoch 15/50\n",
      "25000/25000 [==============================] - 148s - loss: 0.4967 - acc: 0.7616 - val_loss: 0.6843 - val_acc: 0.7004\n",
      "Epoch 16/50\n",
      "25000/25000 [==============================] - 147s - loss: 0.4404 - acc: 0.8050 - val_loss: 0.6185 - val_acc: 0.7134\n",
      "Epoch 17/50\n",
      "25000/25000 [==============================] - 129s - loss: 0.4785 - acc: 0.7766 - val_loss: 0.5922 - val_acc: 0.7209\n",
      "Epoch 18/50\n",
      "25000/25000 [==============================] - 112s - loss: 0.4272 - acc: 0.8101 - val_loss: 0.6247 - val_acc: 0.7222\n",
      "Epoch 19/50\n",
      "25000/25000 [==============================] - 127s - loss: 0.3766 - acc: 0.8414 - val_loss: 0.5599 - val_acc: 0.7160\n",
      "Epoch 20/50\n",
      "25000/25000 [==============================] - 100s - loss: 0.3695 - acc: 0.8457 - val_loss: 0.6219 - val_acc: 0.7312\n",
      "Epoch 21/50\n",
      "25000/25000 [==============================] - 79s - loss: 0.3555 - acc: 0.8588 - val_loss: 0.6518 - val_acc: 0.7270\n",
      "Epoch 22/50\n",
      "25000/25000 [==============================] - 123s - loss: 0.3981 - acc: 0.8254 - val_loss: 0.6062 - val_acc: 0.7015\n",
      "Epoch 23/50\n",
      "25000/25000 [==============================] - 80s - loss: 0.3715 - acc: 0.8376 - val_loss: 0.6146 - val_acc: 0.7210\n",
      "Epoch 24/50\n",
      "25000/25000 [==============================] - 100s - loss: 0.3806 - acc: 0.8321 - val_loss: 0.6397 - val_acc: 0.7202\n",
      "Epoch 25/50\n",
      "25000/25000 [==============================] - 82s - loss: 0.3566 - acc: 0.8499 - val_loss: 0.6504 - val_acc: 0.7172\n",
      "Epoch 26/50\n",
      "25000/25000 [==============================] - 72s - loss: 0.3056 - acc: 0.8812 - val_loss: 0.6548 - val_acc: 0.7411\n",
      "Epoch 27/50\n",
      "25000/25000 [==============================] - 83s - loss: 0.3173 - acc: 0.8738 - val_loss: 0.6324 - val_acc: 0.7139\n",
      "Epoch 28/50\n",
      "25000/25000 [==============================] - 69s - loss: 0.2906 - acc: 0.8860 - val_loss: 0.6453 - val_acc: 0.7248\n",
      "Epoch 29/50\n",
      "25000/25000 [==============================] - 64s - loss: 0.2992 - acc: 0.8816 - val_loss: 0.6776 - val_acc: 0.7271\n",
      "Epoch 30/50\n",
      "25000/25000 [==============================] - 92s - loss: 0.2643 - acc: 0.9004 - val_loss: 0.6685 - val_acc: 0.7176\n",
      "Epoch 31/50\n",
      "25000/25000 [==============================] - 83s - loss: 0.2618 - acc: 0.8988 - val_loss: 0.7150 - val_acc: 0.7402\n",
      "Epoch 32/50\n",
      "25000/25000 [==============================] - 71s - loss: 0.2259 - acc: 0.9167 - val_loss: 0.6786 - val_acc: 0.7309\n",
      "Epoch 33/50\n",
      "25000/25000 [==============================] - 96s - loss: 0.2567 - acc: 0.9037 - val_loss: 0.7147 - val_acc: 0.7204\n",
      "Epoch 34/50\n",
      "25000/25000 [==============================] - 79s - loss: 0.2855 - acc: 0.8842 - val_loss: 0.6679 - val_acc: 0.7350\n",
      "Epoch 35/50\n",
      "25000/25000 [==============================] - 75s - loss: 0.2470 - acc: 0.9062 - val_loss: 0.6934 - val_acc: 0.7274\n",
      "Epoch 36/50\n",
      "25000/25000 [==============================] - 84s - loss: 0.2410 - acc: 0.9110 - val_loss: 0.6904 - val_acc: 0.7322\n",
      "Epoch 37/50\n",
      "25000/25000 [==============================] - 105s - loss: 0.4177 - acc: 0.8214 - val_loss: 0.6682 - val_acc: 0.6913\n",
      "Epoch 38/50\n",
      "25000/25000 [==============================] - 63s - loss: 0.4773 - acc: 0.7764 - val_loss: 0.6633 - val_acc: 0.6666\n",
      "Epoch 39/50\n",
      "25000/25000 [==============================] - 59s - loss: 0.4768 - acc: 0.7686 - val_loss: 0.6593 - val_acc: 0.6694\n",
      "Epoch 40/50\n",
      "25000/25000 [==============================] - 60s - loss: 0.4190 - acc: 0.8115 - val_loss: 0.6708 - val_acc: 0.6916\n",
      "Epoch 41/50\n",
      "25000/25000 [==============================] - 58s - loss: 0.3623 - acc: 0.8532 - val_loss: 0.6627 - val_acc: 0.7012\n",
      "Epoch 42/50\n",
      "25000/25000 [==============================] - 58s - loss: 0.4368 - acc: 0.7920 - val_loss: 0.6490 - val_acc: 0.6769\n",
      "Epoch 43/50\n",
      "25000/25000 [==============================] - 58s - loss: 0.4178 - acc: 0.8074 - val_loss: 0.6522 - val_acc: 0.6961\n",
      "Epoch 44/50\n",
      "25000/25000 [==============================] - 58s - loss: 0.3556 - acc: 0.8558 - val_loss: 0.6697 - val_acc: 0.7179\n",
      "Epoch 45/50\n",
      "25000/25000 [==============================] - 58s - loss: 0.3105 - acc: 0.8822 - val_loss: 0.6624 - val_acc: 0.7252\n",
      "Epoch 46/50\n",
      "25000/25000 [==============================] - 58s - loss: 0.2808 - acc: 0.8990 - val_loss: 0.6806 - val_acc: 0.7276\n",
      "Epoch 47/50\n",
      "25000/25000 [==============================] - 60s - loss: 0.2514 - acc: 0.9106 - val_loss: 0.6917 - val_acc: 0.7294\n",
      "Epoch 48/50\n",
      "25000/25000 [==============================] - 59s - loss: 0.2622 - acc: 0.9063 - val_loss: 0.7164 - val_acc: 0.7256\n",
      "Epoch 49/50\n",
      "25000/25000 [==============================] - 58s - loss: 0.2775 - acc: 0.8962 - val_loss: 0.6965 - val_acc: 0.7293\n",
      "Epoch 50/50\n",
      "25000/25000 [==============================] - 59s - loss: 0.2509 - acc: 0.9116 - val_loss: 0.7032 - val_acc: 0.7284\n",
      "24992/25000 [============================>.] - ETA: 0s\n",
      "Test accuracy: 0.703151030703\n",
      "Test loss: 0.72844\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "\n",
    "# Play with those! try and get better results!\n",
    "model.add(SimpleRNN(128))  \n",
    "#model.add(GRU(128))  \n",
    "#model.add(LSTM(128))  \n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam' ,metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"Train...\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, \n",
    "          nb_epoch=max_epoch, validation_data=(X_test, y_test))\n",
    "acc , loss = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print()\n",
    "print('Test accuracy:', acc)\n",
    "print('Test loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/50\n",
      "25000/25000 [==============================] - 125s - loss: 0.4269 - acc: 0.7995 - val_loss: 0.3627 - val_acc: 0.8408\n",
      "Epoch 2/50\n",
      "25000/25000 [==============================] - 125s - loss: 0.2247 - acc: 0.9136 - val_loss: 0.3649 - val_acc: 0.8539\n",
      "Epoch 3/50\n",
      "25000/25000 [==============================] - 125s - loss: 0.1128 - acc: 0.9606 - val_loss: 0.4526 - val_acc: 0.8387\n",
      "Epoch 4/50\n",
      "25000/25000 [==============================] - 126s - loss: 0.0564 - acc: 0.9818 - val_loss: 0.5723 - val_acc: 0.8390\n",
      "Epoch 5/50\n",
      "25000/25000 [==============================] - 125s - loss: 0.0290 - acc: 0.9908 - val_loss: 0.6926 - val_acc: 0.8381\n",
      "Epoch 6/50\n",
      "25000/25000 [==============================] - 128s - loss: 0.0199 - acc: 0.9936 - val_loss: 0.7457 - val_acc: 0.8370\n",
      "Epoch 7/50\n",
      "25000/25000 [==============================] - 128s - loss: 0.0187 - acc: 0.9940 - val_loss: 0.7592 - val_acc: 0.8323\n",
      "Epoch 8/50\n",
      "25000/25000 [==============================] - 128s - loss: 0.0124 - acc: 0.9963 - val_loss: 0.8159 - val_acc: 0.8336\n",
      "Epoch 9/50\n",
      "25000/25000 [==============================] - 127s - loss: 0.0100 - acc: 0.9970 - val_loss: 0.8686 - val_acc: 0.8337\n",
      "Epoch 10/50\n",
      "25000/25000 [==============================] - 127s - loss: 0.0079 - acc: 0.9976 - val_loss: 1.0417 - val_acc: 0.8314\n",
      "Epoch 11/50\n",
      "25000/25000 [==============================] - 128s - loss: 0.0078 - acc: 0.9981 - val_loss: 1.1780 - val_acc: 0.8273\n",
      "Epoch 12/50\n",
      "25000/25000 [==============================] - 127s - loss: 0.0075 - acc: 0.9977 - val_loss: 1.0101 - val_acc: 0.8305\n",
      "Epoch 13/50\n",
      "25000/25000 [==============================] - 132s - loss: 0.0046 - acc: 0.9987 - val_loss: 1.0592 - val_acc: 0.8306\n",
      "Epoch 14/50\n",
      "25000/25000 [==============================] - 128s - loss: 0.0024 - acc: 0.9995 - val_loss: 1.1763 - val_acc: 0.8256\n",
      "Epoch 15/50\n",
      "25000/25000 [==============================] - 129s - loss: 0.0043 - acc: 0.9984 - val_loss: 1.0740 - val_acc: 0.8182\n",
      "Epoch 16/50\n",
      "25000/25000 [==============================] - 132s - loss: 0.0068 - acc: 0.9977 - val_loss: 1.0961 - val_acc: 0.8257\n",
      "Epoch 17/50\n",
      "25000/25000 [==============================] - 136s - loss: 0.0059 - acc: 0.9981 - val_loss: 1.1141 - val_acc: 0.8211\n",
      "Epoch 18/50\n",
      "25000/25000 [==============================] - 132s - loss: 0.0018 - acc: 0.9993 - val_loss: 1.2348 - val_acc: 0.8264\n",
      "Epoch 19/50\n",
      "25000/25000 [==============================] - 130s - loss: 0.0013 - acc: 0.9997 - val_loss: 1.2530 - val_acc: 0.8228\n",
      "Epoch 20/50\n",
      "25000/25000 [==============================] - 125s - loss: 0.0013 - acc: 0.9997 - val_loss: 1.2916 - val_acc: 0.8239\n",
      "Epoch 21/50\n",
      "25000/25000 [==============================] - 125s - loss: 5.0085e-04 - acc: 0.9999 - val_loss: 1.2925 - val_acc: 0.8235\n",
      "Epoch 22/50\n",
      "25000/25000 [==============================] - 131s - loss: 0.0020 - acc: 0.9995 - val_loss: 1.2111 - val_acc: 0.8130\n",
      "Epoch 23/50\n",
      "25000/25000 [==============================] - 135s - loss: 0.0056 - acc: 0.9981 - val_loss: 1.3544 - val_acc: 0.8205\n",
      "Epoch 24/50\n",
      "25000/25000 [==============================] - 127s - loss: 0.0022 - acc: 0.9995 - val_loss: 1.4701 - val_acc: 0.8184\n",
      "Epoch 25/50\n",
      "25000/25000 [==============================] - 126s - loss: 8.3389e-05 - acc: 1.0000 - val_loss: 1.5009 - val_acc: 0.8182\n",
      "Epoch 26/50\n",
      "25000/25000 [==============================] - 126s - loss: 1.4345e-05 - acc: 1.0000 - val_loss: 1.5521 - val_acc: 0.8190\n",
      "Epoch 27/50\n",
      "25000/25000 [==============================] - 126s - loss: 8.2013e-06 - acc: 1.0000 - val_loss: 1.5949 - val_acc: 0.8193\n",
      "Epoch 28/50\n",
      "25000/25000 [==============================] - 126s - loss: 6.1162e-06 - acc: 1.0000 - val_loss: 1.6475 - val_acc: 0.8189\n",
      "Epoch 29/50\n",
      "25000/25000 [==============================] - 126s - loss: 4.0704e-06 - acc: 1.0000 - val_loss: 1.7000 - val_acc: 0.8186\n",
      "Epoch 30/50\n",
      "25000/25000 [==============================] - 126s - loss: 2.6037e-06 - acc: 1.0000 - val_loss: 1.7523 - val_acc: 0.8189\n",
      "Epoch 31/50\n",
      "25000/25000 [==============================] - 126s - loss: 0.0066 - acc: 0.9976 - val_loss: 1.3865 - val_acc: 0.8150\n",
      "Epoch 32/50\n",
      "25000/25000 [==============================] - 126s - loss: 0.0082 - acc: 0.9975 - val_loss: 1.3139 - val_acc: 0.8201\n",
      "Epoch 33/50\n",
      "25000/25000 [==============================] - 126s - loss: 5.3398e-04 - acc: 0.9998 - val_loss: 1.3897 - val_acc: 0.8155\n",
      "Epoch 34/50\n",
      "25000/25000 [==============================] - 126s - loss: 7.6049e-05 - acc: 1.0000 - val_loss: 1.4792 - val_acc: 0.8188\n",
      "Epoch 35/50\n",
      "25000/25000 [==============================] - 126s - loss: 1.8332e-05 - acc: 1.0000 - val_loss: 1.5524 - val_acc: 0.8184\n",
      "Epoch 36/50\n",
      "25000/25000 [==============================] - 126s - loss: 9.3698e-06 - acc: 1.0000 - val_loss: 1.6106 - val_acc: 0.8177\n",
      "Epoch 37/50\n",
      "25000/25000 [==============================] - 126s - loss: 6.1041e-06 - acc: 1.0000 - val_loss: 1.6386 - val_acc: 0.8182\n",
      "Epoch 38/50\n",
      "25000/25000 [==============================] - 125s - loss: 3.4338e-06 - acc: 1.0000 - val_loss: 1.6852 - val_acc: 0.8183\n",
      "Epoch 39/50\n",
      "25000/25000 [==============================] - 126s - loss: 6.0692e-06 - acc: 1.0000 - val_loss: 1.6937 - val_acc: 0.8180\n",
      "Epoch 40/50\n",
      "25000/25000 [==============================] - 126s - loss: 1.5602e-06 - acc: 1.0000 - val_loss: 1.7547 - val_acc: 0.8158\n",
      "Epoch 41/50\n",
      "25000/25000 [==============================] - 126s - loss: 9.0590e-07 - acc: 1.0000 - val_loss: 1.7959 - val_acc: 0.8157\n",
      "Epoch 42/50\n",
      "25000/25000 [==============================] - 126s - loss: 7.4583e-07 - acc: 1.0000 - val_loss: 1.8433 - val_acc: 0.8153\n",
      "Epoch 43/50\n",
      "25000/25000 [==============================] - 130s - loss: 5.4697e-07 - acc: 1.0000 - val_loss: 1.8860 - val_acc: 0.8148\n",
      "Epoch 44/50\n",
      "25000/25000 [==============================] - 128s - loss: 4.2834e-07 - acc: 1.0000 - val_loss: 1.9190 - val_acc: 0.8145\n",
      "Epoch 45/50\n",
      "25000/25000 [==============================] - 128s - loss: 3.0582e-07 - acc: 1.0000 - val_loss: 1.9616 - val_acc: 0.8148\n",
      "Epoch 46/50\n",
      "25000/25000 [==============================] - 128s - loss: 2.6837e-07 - acc: 1.0000 - val_loss: 2.0036 - val_acc: 0.8147\n",
      "Epoch 47/50\n",
      "25000/25000 [==============================] - 128s - loss: 2.0370e-07 - acc: 1.0000 - val_loss: 2.0401 - val_acc: 0.8141\n",
      "Epoch 48/50\n",
      "25000/25000 [==============================] - 128s - loss: 1.6904e-07 - acc: 1.0000 - val_loss: 2.0771 - val_acc: 0.8138\n",
      "Epoch 49/50\n",
      "25000/25000 [==============================] - 129s - loss: 1.4488e-07 - acc: 1.0000 - val_loss: 2.1262 - val_acc: 0.8133\n",
      "Epoch 50/50\n",
      "25000/25000 [==============================] - 129s - loss: 1.4191e-07 - acc: 1.0000 - val_loss: 2.1325 - val_acc: 0.8141\n",
      "25000/25000 [==============================] - 26s    \n",
      "\n",
      "Test accuracy: 2.13254456764\n",
      "Test loss: 0.81412\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "\n",
    "# Play with those! try and get better results!\n",
    "# model.add(SimpleRNN(128))  \n",
    "model.add(GRU(128))  \n",
    "# model.add(LSTM(128))  \n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam' ,metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"Train...\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, \n",
    "          nb_epoch=max_epoch, validation_data=(X_test, y_test))\n",
    "acc , loss = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print()\n",
    "print('Test accuracy:', acc)\n",
    "print('Test loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/50\n",
      "25000/25000 [==============================] - 137s - loss: 0.4363 - acc: 0.7973 - val_loss: 0.3679 - val_acc: 0.8391\n",
      "Epoch 2/50\n",
      "25000/25000 [==============================] - 136s - loss: 0.2392 - acc: 0.9094 - val_loss: 0.4281 - val_acc: 0.8330\n",
      "Epoch 3/50\n",
      "25000/25000 [==============================] - 140s - loss: 0.1404 - acc: 0.9487 - val_loss: 0.5021 - val_acc: 0.8389\n",
      "Epoch 4/50\n",
      "25000/25000 [==============================] - 139s - loss: 0.0782 - acc: 0.9743 - val_loss: 0.5590 - val_acc: 0.8310\n",
      "Epoch 5/50\n",
      "25000/25000 [==============================] - 138s - loss: 0.0551 - acc: 0.9814 - val_loss: 0.7147 - val_acc: 0.8321\n",
      "Epoch 6/50\n",
      "25000/25000 [==============================] - 137s - loss: 0.0370 - acc: 0.9881 - val_loss: 0.8151 - val_acc: 0.8163\n",
      "Epoch 7/50\n",
      "25000/25000 [==============================] - 146s - loss: 0.0253 - acc: 0.9922 - val_loss: 0.7970 - val_acc: 0.8276\n",
      "Epoch 8/50\n",
      "25000/25000 [==============================] - 136s - loss: 0.0231 - acc: 0.9926 - val_loss: 0.8430 - val_acc: 0.8294\n",
      "Epoch 9/50\n",
      "25000/25000 [==============================] - 135s - loss: 0.0163 - acc: 0.9949 - val_loss: 0.8001 - val_acc: 0.8260\n",
      "Epoch 10/50\n",
      "25000/25000 [==============================] - 133s - loss: 0.0122 - acc: 0.9963 - val_loss: 0.8293 - val_acc: 0.8258\n",
      "Epoch 11/50\n",
      "25000/25000 [==============================] - 134s - loss: 0.0166 - acc: 0.9946 - val_loss: 0.8438 - val_acc: 0.8316\n",
      "Epoch 12/50\n",
      "25000/25000 [==============================] - 132s - loss: 0.0123 - acc: 0.9962 - val_loss: 0.8786 - val_acc: 0.8262\n",
      "Epoch 13/50\n",
      "25000/25000 [==============================] - 136s - loss: 0.0094 - acc: 0.9970 - val_loss: 0.9157 - val_acc: 0.8279\n",
      "Epoch 14/50\n",
      "25000/25000 [==============================] - 136s - loss: 0.0068 - acc: 0.9984 - val_loss: 1.0131 - val_acc: 0.8256\n",
      "Epoch 15/50\n",
      "25000/25000 [==============================] - 133s - loss: 0.0060 - acc: 0.9982 - val_loss: 1.0259 - val_acc: 0.8254\n",
      "Epoch 16/50\n",
      "25000/25000 [==============================] - 137s - loss: 0.0104 - acc: 0.9965 - val_loss: 1.0047 - val_acc: 0.8246\n",
      "Epoch 17/50\n",
      "25000/25000 [==============================] - 134s - loss: 0.0062 - acc: 0.9983 - val_loss: 1.1404 - val_acc: 0.8173\n",
      "Epoch 18/50\n",
      "25000/25000 [==============================] - 136s - loss: 0.0101 - acc: 0.9971 - val_loss: 1.0136 - val_acc: 0.8176\n",
      "Epoch 19/50\n",
      "25000/25000 [==============================] - 136s - loss: 0.0018 - acc: 0.9997 - val_loss: 1.2374 - val_acc: 0.8190\n",
      "Epoch 20/50\n",
      "25000/25000 [==============================] - 136s - loss: 3.5442e-04 - acc: 0.9999 - val_loss: 1.3023 - val_acc: 0.8102\n",
      "Epoch 21/50\n",
      "25000/25000 [==============================] - 134s - loss: 0.0062 - acc: 0.9981 - val_loss: 0.9741 - val_acc: 0.8176\n",
      "Epoch 22/50\n",
      "25000/25000 [==============================] - 135s - loss: 0.0109 - acc: 0.9965 - val_loss: 1.1147 - val_acc: 0.8278\n",
      "Epoch 23/50\n",
      "25000/25000 [==============================] - 140s - loss: 0.0056 - acc: 0.9985 - val_loss: 1.0957 - val_acc: 0.8168\n",
      "Epoch 24/50\n",
      "25000/25000 [==============================] - 133s - loss: 0.0020 - acc: 0.9995 - val_loss: 1.2580 - val_acc: 0.8221\n",
      "Epoch 25/50\n",
      "25000/25000 [==============================] - 135s - loss: 0.0024 - acc: 0.9992 - val_loss: 1.1497 - val_acc: 0.8177\n",
      "Epoch 26/50\n",
      "25000/25000 [==============================] - 134s - loss: 3.8771e-04 - acc: 0.9999 - val_loss: 1.3355 - val_acc: 0.8174\n",
      "Epoch 27/50\n",
      "25000/25000 [==============================] - 136s - loss: 7.8988e-05 - acc: 1.0000 - val_loss: 1.4460 - val_acc: 0.8186\n",
      "Epoch 28/50\n",
      "25000/25000 [==============================] - 136s - loss: 2.3978e-05 - acc: 1.0000 - val_loss: 1.5422 - val_acc: 0.8184\n",
      "Epoch 29/50\n",
      "25000/25000 [==============================] - 136s - loss: 1.1710e-05 - acc: 1.0000 - val_loss: 1.6239 - val_acc: 0.8187\n",
      "Epoch 30/50\n",
      "25000/25000 [==============================] - 135s - loss: 8.0861e-06 - acc: 1.0000 - val_loss: 1.6800 - val_acc: 0.8184\n",
      "Epoch 31/50\n",
      "25000/25000 [==============================] - 137s - loss: 4.2025e-06 - acc: 1.0000 - val_loss: 1.7640 - val_acc: 0.8184\n",
      "Epoch 32/50\n",
      "25000/25000 [==============================] - 136s - loss: 3.9046e-06 - acc: 1.0000 - val_loss: 1.8178 - val_acc: 0.8184\n",
      "Epoch 33/50\n",
      "25000/25000 [==============================] - 132s - loss: 1.9572e-06 - acc: 1.0000 - val_loss: 1.8909 - val_acc: 0.8184\n",
      "Epoch 34/50\n",
      "25000/25000 [==============================] - 136s - loss: 1.0967e-06 - acc: 1.0000 - val_loss: 1.9291 - val_acc: 0.8180\n",
      "Epoch 35/50\n",
      "25000/25000 [==============================] - 136s - loss: 6.9052e-07 - acc: 1.0000 - val_loss: 1.9856 - val_acc: 0.8173\n",
      "Epoch 36/50\n",
      "25000/25000 [==============================] - 133s - loss: 0.0095 - acc: 0.9974 - val_loss: 1.2293 - val_acc: 0.8137\n",
      "Epoch 37/50\n",
      "25000/25000 [==============================] - 133s - loss: 0.0163 - acc: 0.9953 - val_loss: 1.0759 - val_acc: 0.8175\n",
      "Epoch 38/50\n",
      "25000/25000 [==============================] - 133s - loss: 0.0047 - acc: 0.9990 - val_loss: 1.1115 - val_acc: 0.8173\n",
      "Epoch 39/50\n",
      "25000/25000 [==============================] - 135s - loss: 7.7323e-04 - acc: 0.9998 - val_loss: 1.1998 - val_acc: 0.8176\n",
      "Epoch 40/50\n",
      "25000/25000 [==============================] - 133s - loss: 1.1975e-04 - acc: 1.0000 - val_loss: 1.3469 - val_acc: 0.8200\n",
      "Epoch 41/50\n",
      "25000/25000 [==============================] - 137s - loss: 4.7689e-05 - acc: 1.0000 - val_loss: 1.4118 - val_acc: 0.8197\n",
      "Epoch 42/50\n",
      "25000/25000 [==============================] - 136s - loss: 4.1476e-05 - acc: 1.0000 - val_loss: 1.4341 - val_acc: 0.8191\n",
      "Epoch 43/50\n",
      "25000/25000 [==============================] - 135s - loss: 1.9887e-05 - acc: 1.0000 - val_loss: 1.5386 - val_acc: 0.8190\n",
      "Epoch 44/50\n",
      "25000/25000 [==============================] - 139s - loss: 1.1163e-05 - acc: 1.0000 - val_loss: 1.6324 - val_acc: 0.8176\n",
      "Epoch 45/50\n",
      "25000/25000 [==============================] - 135s - loss: 7.4072e-06 - acc: 1.0000 - val_loss: 1.6605 - val_acc: 0.8191\n",
      "Epoch 46/50\n",
      "25000/25000 [==============================] - 137s - loss: 5.7761e-06 - acc: 1.0000 - val_loss: 1.7437 - val_acc: 0.8180\n",
      "Epoch 47/50\n",
      "25000/25000 [==============================] - 134s - loss: 2.6823e-06 - acc: 1.0000 - val_loss: 1.8282 - val_acc: 0.8174\n",
      "Epoch 48/50\n",
      "25000/25000 [==============================] - 147s - loss: 1.7478e-06 - acc: 1.0000 - val_loss: 1.8321 - val_acc: 0.8164\n",
      "Epoch 49/50\n",
      "25000/25000 [==============================] - 140s - loss: 1.0350e-06 - acc: 1.0000 - val_loss: 1.9065 - val_acc: 0.8172\n",
      "Epoch 50/50\n",
      "25000/25000 [==============================] - 138s - loss: 0.0071 - acc: 0.9979 - val_loss: 1.1281 - val_acc: 0.8025\n",
      "25000/25000 [==============================] - 34s    \n",
      "\n",
      "Test accuracy: 1.12805138825\n",
      "Test loss: 0.80248\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "\n",
    "# Play with those! try and get better results!\n",
    "# model.add(SimpleRNN(128))  \n",
    "# model.add(GRU(128))  \n",
    "model.add(LSTM(128))  \n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam' ,metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"Train...\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, \n",
    "          nb_epoch=max_epoch, validation_data=(X_test, y_test))\n",
    "acc , loss = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print()\n",
    "print('Test accuracy:', acc)\n",
    "print('Test loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Generation using RNN(LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600893\n",
      "total chars: 57\n",
      "nb sequences: 200285\n",
      "Vectorization...\n",
      "Build model...\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Epoch 1/1\n",
      "200285/200285 [==============================] - 314s - loss: 1.9922   \n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"n short, more pleasantly--one is much mo\"\n",
      "n short, more pleasantly--one is much most the stread the conticion of the contime the condeption of the depreted of the deperions and which it is called the most the self the strean the most the continity and the word to be a cause of the conticion of the contime the aster the stare of the conticion of the conticion of the contime the love the word the deperient the some and into the same the hand and a propers and a stread and a stare\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Epoch 1/1\n",
      "200285/200285 [==============================] - 310s - loss: 1.6420   \n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"le certainty with which his instinct tre\"\n",
      "le certainty with which his instinct tread to the suppore to such a stronger and such an all the such and the sense and the species of the stronger to the will to the such an actions and and and the same time of the stark of the such an an a stronger to an and in the wishes to have the such and such a problem of the such a desire and and the such and and and such one to be the present that it is not of the such an and the such and the s\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Epoch 1/1\n",
      "200285/200285 [==============================] - 313s - loss: 1.5463   \n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"crudest form. we\n",
      "organic beings are prim\"\n",
      "crudest form. we\n",
      "organic beings are primorous to the stands of the words of the stands and the world of the stands of the stands of the morals, and he will to the same and the seems of the self-desperion of the free instinct of the fact of the problem of the self-of the responsible the same with the respect of the respect of the not the same of the self-desting the same and instinction of the stands the words of the same the same the wo\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Epoch 1/1\n",
      "200285/200285 [==============================] - 317s - loss: 1.4995   \n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"gely. the\n",
      "question is, how far an opinio\"\n",
      "gely. the\n",
      "question is, how far an opinion the strived the strived the strived the strived the strived the strived the strived the strived the strived to the strived the strived the strived the spectained the strived the strived the strived and say the strived the strived to the powerful and the strived the strived the strived the strived the strived and sometimes of the strived the strived the strived the strived the strived the strived\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "Epoch 1/1\n",
      "200285/200285 [==============================] - 319s - loss: 1.4694   \n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"d justice is very great. i believe that\n",
      "\"\n",
      "d justice is very great. i believe that\n",
      "all the probably a sublems to the world and man of the probably the conscious and the world and the promotion of the probably and in the probably the great first and man who have been the self-and and and the precisely the promotion of the fact the probably the self-and the probably the self-and appear and are also the promotion of the factive man who have a present the same the morals of the prob\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 6\n",
      "Epoch 1/1\n",
      "200285/200285 [==============================] - 309s - loss: 1.4467   \n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"e concealed god, in the\n",
      "'thing-in-itself\"\n",
      "e concealed god, in the\n",
      "'thing-in-itself and the subject and spirit, and and the self-even the procession of the same of the soung in the sense of the sensible which is the strength of the same the subject and for the strength, and something and same of the sense of the sense of the subject and subject of the same to the same to the sound to the strength of the subject the self-even the strength of the process of the presertion of the e\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 7\n",
      "Epoch 1/1\n",
      "200285/200285 [==============================] - 315s - loss: 1.4299   \n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"r fact remains, however, that everything\"\n",
      "r fact remains, however, that everything the state of the state of the state of the soul, the stricted to the consequently the expression, and the comparison of the morality and as the serious spirits of the morality and and a state of the state of the state of the senses and in the procoming the state of the real state of the result to the state of the morality of the world of the procoming and a state of the same time of the soul of t\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 8\n",
      "Epoch 1/1\n",
      "200285/200285 [==============================] - 314s - loss: 1.4184   \n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"mong men.\n",
      "\n",
      "220. now that the praise of t\"\n",
      "mong men.\n",
      "\n",
      "220. now that the praise of the significant the sense--or it is a subject of the senses of the conception of the sense--the sentiment of the sentiments of the present of the standard to the things and what a sense of the world of the most could not be in the same time and the sense of the sentual and the same time in the sense of the same the strength of the sentiment of the same the same contrary and inderst of the sentiment\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 9\n",
      "Epoch 1/1\n",
      "200285/200285 [==============================] - 303s - loss: 1.4076   \n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"thing more than\n",
      "\"prudently and apart.\" w\"\n",
      "thing more than\n",
      "\"prudently and apart.\" with the same to the sense of the same to the same to himself to the propority of the same to the same time of the strive with the same to the same to the same to the starm the promition of the most the same to the same time of the same time of the same time of the fact the same with which the same to the promition of the most propority of the problem of the same the problem of the same time of the\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 10\n",
      "Epoch 1/1\n",
      "200285/200285 [==============================] - 312s - loss: 1.3980   \n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"tion than if we had done the deed oursel\"\n",
      "tion than if we had done the deed ourselves and whom is the same time in the same work of the same former the same to the proctorical and who has a really art of the sense of the work and something the same work of the same of the subject the sense of the subject and the sense of properating the same to him the discovery and as the same will to the higher and the same to the problemenadly the same to be problemenant and as the more and \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 11\n",
      "Epoch 1/1\n",
      "200285/200285 [==============================] - 322s - loss: 1.3889   \n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"\" against such a \"should\n",
      "be,\" however, t\"\n",
      "\" against such a \"should\n",
      "be,\" however, the proctive and proper to the same to the process of the most the former and proction of the proctive proper to the same to the conscience and more in the same the discondition of the soul in the soul of the proctions and proper of the same the moral in the proction of the conception of the same to the proction of the former and properation of the philosophy and properation of t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/deep-learning/lib/python3.5/site-packages/ipykernel/__main__.py:52: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he world, and prese\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 12\n",
      "Epoch 1/1\n",
      "200285/200285 [==============================] - 312s - loss: 1.3828   \n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"bout the good, true, and beautiful,\n",
      "and \"\n",
      "bout the good, true, and beautiful,\n",
      "and in the same time as the soul of the soul and in the sense of the sense of the most and stronger and constitute that the sense of the sentiment of the soul as the same out of the soul as a something that it is the self--it is also the soul as the soul and the soul as its souls and interpretation, and the same with the sense of the sense of which the sense of the sense of the sense of the soul and a\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 13\n",
      "Epoch 1/1\n",
      "200285/200285 [==============================] - 313s - loss: 1.3764   \n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"le doses is very great,\n",
      "nevertheless; th\"\n",
      "le doses is very great,\n",
      "nevertheless; there is all the sensation of the sensation of the sensations of the survers with the sensations of the sensation of the sensations and experiment the sure in the sensations of the such a strength of the sensation of the supersion of the sensations of the sensations of the same and the survers of the sense of the sensations of the master and the sensation of the sensation of the superstitive and the\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 14\n",
      "Epoch 1/1\n",
      " 28032/200285 [===>..........................] - ETA: 274s - loss: 1.3355"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(path).read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(1, 60):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X, y, batch_size=128, nb_epoch=1)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
